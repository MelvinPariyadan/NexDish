{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nimport torch\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom torchvision import transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 32\nIMG_SIZE   = 256\nNUM_CLASSES = 101\nNUM_EPOCHS  = 5\nLR          = 1e-4\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:15:14.279515Z","iopub.execute_input":"2025-04-22T21:15:14.280562Z","iopub.status.idle":"2025-04-22T21:15:14.329153Z","shell.execute_reply.started":"2025-04-22T21:15:14.280529Z","shell.execute_reply":"2025-04-22T21:15:14.328335Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset = load_dataset(\"food101\")\n#dataset = dataset.filter(lambda ex: ex[\"label\"] < NUM_CLASSES)\n\ntrain_ds = dataset[\"train\"]\nval_ds   = dataset[\"validation\"]\n\n\npreprocess = transforms.Compose([\n    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n       mean=[0.485, 0.456, 0.406],   # ImageNet stats\n       std =[0.229, 0.224, 0.225],\n    ),\n])\n\ndef preprocess_example(example):\n    example['pixel_values'] = preprocess(example['image'])\n    example['labels']       = example['label']\n    return example\n\ntrain_ds = train_ds.map(\n    preprocess_example,\n    remove_columns=['image','label'],\n)\nval_ds = val_ds.map(\n    preprocess_example,\n    remove_columns=['image','label'],\n)\n\ntrain_ds.set_format(type='torch', columns=['pixel_values','labels'])\nval_ds.set_format(type='torch', columns=['pixel_values','labels'])\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:15:14.330567Z","iopub.execute_input":"2025-04-22T21:15:14.331305Z","execution_failed":"2025-04-22T21:33:48.640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/75750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2261738242ae4258aa19989c5c678773"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8076ccfb2834f2e8e14b72ad3455877"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"model = models.resnet50(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, NUM_CLASSES)\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=LR)  # only fine-tuning fc\n\ntrain_acc_list = []\nval_acc_list   = []\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    # Training\n    model.train()\n    correct_train = 0\n    total_train   = 0\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n        imgs   = batch['pixel_values'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss    = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(dim=1)\n        correct_train += (preds == labels).sum().item()\n        total_train   += labels.size(0)\n\n    train_acc = correct_train / total_train\n    train_acc_list.append(train_acc)\n\n    # Validation\n    model.eval()\n    correct_val = 0\n    total_val   = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validate Epoch {epoch}\"):\n            imgs   = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(imgs)\n            preds   = outputs.argmax(dim=1)\n            correct_val += (preds == labels).sum().item()\n            total_val   += labels.size(0)\n\n    val_acc = correct_val / total_val\n    val_acc_list.append(val_acc)\n\n    print(f\"Epoch {epoch}/{NUM_EPOCHS} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n# ── 6) Plotting ──\nepochs = np.arange(1, NUM_EPOCHS + 1)\n\nplt.figure()\nplt.plot(epochs, train_acc_list, label='Train Accuracy')\nplt.plot(epochs, val_acc_list, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training vs. Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T21:33:48.641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}